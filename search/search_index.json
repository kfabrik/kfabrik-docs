{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"KFabrik Documentation","text":"<p>Welcome to KFabrik, an open-source developer and CI tool that eliminates the complexity barrier in cloud-native AI development.</p>"},{"location":"#what-is-kfabrik","title":"What is KFabrik?","text":"<p>KFabrik enables developers to spin up complete, production-like CNCF AI stacks\u2014including Kubernetes, KServe, Ray, vLLM, and vector databases\u2014on their laptops or in CI pipelines in minutes instead of weeks.</p>"},{"location":"#the-problem-we-solve","title":"The Problem We Solve","text":"<p>Today's cloud-native AI platforms are a fragmented patchwork of specialized tools. While production-grade technologies like KServe, Ray, and vLLM exist, integrating them into a cohesive stack creates massive complexity. Organizations spend weeks wiring together Kubernetes, service meshes, model servers, vector databases, and observability tools\u2014only to find that versions conflict, configurations break, and \"it works on my laptop\" becomes \"it fails in CI.\"</p>"},{"location":"#our-solution","title":"Our Solution","text":"<p>KFabrik provides pre-integrated, tested cloud-native AI stacks with:</p> <ul> <li>Unified Integration: Compatible versions of KServe, Ray, vLLM, and vector databases that work together out-of-the-box</li> <li>Fast Setup: Complete CNAI environments in 15-30 minutes instead of weeks</li> <li>Reproducible Environments: Identical configurations across dev, CI, and production-like environments</li> <li>Opinionated Defaults: Production-ready configurations based on CNCF best practices</li> <li>Full Customization: Every aspect is configurable through declarative configuration files</li> </ul>"},{"location":"#core-components","title":"Core Components","text":"<p>KFabrik comprises three core components:</p> <ul> <li>kfabric-bootstrap: Bootstraps complete, integrated CNAI stacks with a single command</li> <li>kfabric-runtime: Orchestrates runtime wiring, health checks, and service discovery across components</li> <li>kfabric-control: Provides declarative APIs for stack management and automation</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>KFabrik is currently in early development. The software components (kfabric-bootstrap, kfabric-runtime, kfabric-control) will be released in the coming weeks. This documentation site is being prepared to support the upcoming launch.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Setup and installation guide (coming soon)</li> <li>PRFAQ - Detailed press release and FAQ</li> <li>Contributing - How to contribute to the project</li> </ul>"},{"location":"#built-for-the-cncf-ecosystem","title":"Built for the CNCF Ecosystem","text":"<p>KFabrik is designed as a potential CNCF project, directly addressing the integration and cohesion gap identified in the CNCF AI landscape. Built on cloud-native principles with declarative configuration, reproducible builds, secure defaults, and zero vendor lock-in.</p>"},{"location":"cncf-landscape/","title":"CNCF AI Landscape","text":"<p>KFabrik is part of the broader Cloud Native Computing Foundation (CNCF) ecosystem focused on AI and machine learning workloads. This page provides context on related projects and how they work together.</p>"},{"location":"cncf-landscape/#related-cncf-projects","title":"Related CNCF Projects","text":""},{"location":"cncf-landscape/#model-serving-inference","title":"Model Serving &amp; Inference","text":"<p>KServe Kubernetes-native model serving platform for machine learning models. Provides serverless inference with autoscaling, canary deployments, and multi-framework support.</p> <p>vLLM High-throughput and memory-efficient inference engine for large language models. Optimized for serving LLMs at scale with advanced batching techniques.</p>"},{"location":"cncf-landscape/#orchestration-management","title":"Orchestration &amp; Management","text":"<p>Karmada Multi-cluster Kubernetes management system that enables workload distribution across multiple clusters, useful for distributed AI training and inference.</p> <p>LLMD Kubernetes operator for managing large language model deployments and lifecycle operations.</p>"},{"location":"cncf-landscape/#how-kfabrik-fits-in","title":"How KFabrik Fits In","text":"<p>KFabrik complements these projects by:</p> <ul> <li>Simplifying deployment of AI workloads across the CNCF ecosystem</li> <li>Providing abstractions that work with KServe, vLLM, and other inference engines</li> <li>Enabling multi-cluster AI workflows through integration with Karmada</li> <li>Democratizing access to cloud-native AI tools for developers</li> </ul>"},{"location":"cncf-landscape/#integration-patterns","title":"Integration Patterns","text":""},{"location":"cncf-landscape/#with-kserve","title":"With KServe","text":"<p>KFabrik can deploy models that are served by KServe, providing a simplified interface for model deployment and management.</p>"},{"location":"cncf-landscape/#with-vllm","title":"With vLLM","text":"<p>Use KFabrik to orchestrate vLLM deployments for high-performance LLM serving across multiple environments.</p>"},{"location":"cncf-landscape/#with-karmada","title":"With Karmada","text":"<p>Leverage Karmada's multi-cluster capabilities to distribute KFabrik-managed AI workloads across different regions or cloud providers.</p>"},{"location":"cncf-landscape/#learn-more","title":"Learn More","text":"<ul> <li>CNCF AI Working Group</li> <li>CNCF Landscape - AI/ML Category</li> <li>KServe GitHub Organization</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>KFabrik is an open-source project and welcomes contributions from the community. As the project is in early development, now is a great time to get involved and help shape its direction.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Improve guides and tutorials</li> <li>Add examples and use cases</li> <li>Fix typos and clarify explanations</li> <li>Translate content</li> </ul>"},{"location":"contributing/#code-coming-soon","title":"Code (Coming Soon)","text":"<p>Once the software is released, you can contribute: - Features and enhancements - Bug fixes - Integration tests - Performance optimizations</p>"},{"location":"contributing/#community","title":"Community","text":"<ul> <li>Answer questions from other users</li> <li>Share your use cases and feedback</li> <li>Present at meetups or conferences</li> <li>Help with testing and validation</li> </ul>"},{"location":"contributing/#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"contributing/#writing-style","title":"Writing Style","text":"<ul> <li>Use clear, concise language</li> <li>Write in present tense</li> <li>Use active voice when possible</li> <li>Keep paragraphs short and focused</li> </ul>"},{"location":"contributing/#markdown-formatting","title":"Markdown Formatting","text":"<ul> <li>Use proper heading hierarchy (h1 \u2192 h2 \u2192 h3)</li> <li>Include code blocks with language identifiers for syntax highlighting</li> <li>Use relative links for internal documentation references</li> <li>Add alt text for images</li> </ul>"},{"location":"contributing/#code-examples","title":"Code Examples","text":"<p>When the software is available: - Provide complete, working examples - Include expected output - Explain what the code does - Use realistic but simple scenarios</p>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/your-feature-name</code>)</li> <li>Make your changes</li> <li>Test locally with <code>mkdocs serve</code></li> <li>Commit with clear, descriptive messages</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#file-organization","title":"File Organization","text":"<ul> <li>Place all documentation in the <code>docs/</code> directory</li> <li>Use lowercase filenames with hyphens (e.g., <code>getting-started.md</code>)</li> <li>Update <code>mkdocs.yml</code> navigation when adding new pages</li> <li>Keep related content together</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your changes follow the documentation standards</li> <li>Update the navigation in <code>mkdocs.yml</code> if adding new pages</li> <li>Test that all links work correctly</li> <li>Provide a clear description of your changes</li> <li>Reference any related issues</li> </ol>"},{"location":"contributing/#community-resources","title":"Community Resources","text":"<ul> <li>GitHub: github.com/kfabrik</li> <li>Community: github.com/kfabrik/community</li> <li>Slack: Coming soon</li> <li>Mailing List: Coming soon</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>KFabrik is committed to providing a welcoming and inclusive environment. All contributors are expected to:</p> <ul> <li>Be respectful and considerate</li> <li>Welcome newcomers and help them get started</li> <li>Focus on constructive feedback</li> <li>Respect differing viewpoints and experiences</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to KFabrik, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>If you have questions about contributing, please open an issue on GitHub or reach out through community channels.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>KFabrik is currently in early development. The software will be released in the coming weeks.</p>"},{"location":"getting-started/#system-requirements","title":"System Requirements","text":"<p>When released, KFabrik will require:</p>"},{"location":"getting-started/#minimum-basic-kserve-development","title":"Minimum (Basic KServe Development)","text":"<ul> <li>CPU: 4 cores (8 recommended)</li> <li>RAM: 8GB (16GB recommended)</li> <li>Disk: 20GB free space</li> <li>OS: Linux, macOS, or Windows with WSL2</li> <li>Software: Docker or Podman</li> </ul>"},{"location":"getting-started/#recommended-ray-clusters-or-llm-serving","title":"Recommended (Ray Clusters or LLM Serving)","text":"<ul> <li>CPU: 8+ cores</li> <li>RAM: 32GB+</li> <li>GPU: Optional but recommended for LLM inference testing</li> </ul>"},{"location":"getting-started/#quick-start-coming-soon","title":"Quick Start (Coming Soon)","text":"<p>Once released, getting started will be simple:</p> <pre><code># Install KFabrik\npip install kfabrik\n\n# Bootstrap a complete KServe stack\nkfabrik bootstrap --profile kserve-basic\n\n# Verify the stack is running\nkfabrik status\n</code></pre> <p>A complete KServe-based inference environment will be running locally in under 15 minutes.</p>"},{"location":"getting-started/#what-youll-get","title":"What You'll Get","text":"<p>A fully functional cloud-native AI environment including:</p> <ul> <li>Kubernetes cluster (kind or k3s)</li> <li>KServe with Istio/Knative</li> <li>Sample model deployment</li> <li>Observability tools (optional)</li> </ul>"},{"location":"getting-started/#stack-profiles","title":"Stack Profiles","text":"<p>KFabrik will support multiple stack profiles:</p> <ul> <li>kserve-basic: KServe with minimal dependencies for model serving</li> <li>kserve-ray: KServe + Ray for distributed workloads</li> <li>rag-stack: KServe + vector database for RAG patterns</li> <li>full-stack: Complete CNAI stack with all components</li> </ul>"},{"location":"getting-started/#documentation-development","title":"Documentation Development","text":"<p>This documentation site is built with MkDocs. To contribute to the documentation:</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>pip</li> </ul>"},{"location":"getting-started/#local-development","title":"Local Development","text":"<pre><code># Install dependencies\npip install -r requirements.txt\n\n# Serve documentation locally\nmkdocs serve\n</code></pre> <p>Visit <code>http://127.0.0.1:8000</code> to view the documentation locally.</p>"},{"location":"getting-started/#stay-updated","title":"Stay Updated","text":"<ul> <li>GitHub: github.com/kfabrik</li> <li>Documentation: kfabrik.io</li> </ul> <p>Watch the repository for updates on the initial release.</p>"},{"location":"prfaq/","title":"KFabrik PRFAQ","text":""},{"location":"prfaq/#press-release","title":"PRESS RELEASE","text":""},{"location":"prfaq/#kfabrik-launches-open-source-tool-to-democratize-cloud-native-ai-development","title":"KFabrik Launches Open-Source Tool to Democratize Cloud-Native AI Development","text":"<p>Developer and CI tool eliminates weeks of setup, delivers production-like CNCF AI stacks in minutes</p> <p>TAMPA, FL \u2014 January 31, 2026 \u2014 The KFabrik project today announced the release of its open-source developer and continuous integration (CI) tool designed to eliminate the complexity barrier preventing teams from building and testing cloud-native AI applications. KFabrik enables developers to spin up complete, production-like CNCF AI stacks\u2014including Kubernetes, KServe, Ray, vLLM, and vector databases\u2014on their laptops or in CI pipelines in minutes instead of weeks.</p> <p>The Problem: A Patchwork of Tools Creates Complexity and Risk</p> <p>Today's cloud-native AI platforms are a fragmented patchwork of specialized tools. While production-grade technologies like KServe, Ray, and vLLM exist, integrating them into a cohesive stack creates massive complexity. Organizations spend weeks wiring together Kubernetes, service meshes, model servers, vector databases, and observability tools\u2014only to find that versions conflict, configurations break, and \"it works on my laptop\" becomes \"it fails in CI.\"</p> <p>This fragmentation means higher complexity and risk. Data scientists develop locally with small datasets, then distributed systems engineers rewrite code for production Kubernetes deployments. Teams lack standard, tested configurations and spend engineering cycles solving the same integration problems independently. Without a unified approach, the learning curve is steep, requiring expertise in both ML and cloud-native infrastructure.</p> <p>\"We saw talented teams spending 40% of their sprint cycles fighting tool integration instead of building AI features,\" said a KFabrik contributor. \"Every project started from scratch\u2014figuring out which KServe version works with which Istio release, how to wire Ray to vector databases, how to get vLLM serving working locally. The CNCF ecosystem provides incredible building blocks, but assembling them is still artisanal, error-prone work.\"</p> <p>The Solution: Unified, Tested CNAI Stacks</p> <p>KFabrik solves the integration and cohesion problem by providing pre-integrated, tested cloud-native AI stacks. Instead of assembling KServe, Ray, vLLM, and vector databases independently, developers get working, compatible configurations in one command. KFabrik handles version compatibility, component wiring, and infrastructure dependencies\u2014eliminating weeks of integration work.</p> <p>Built as a unified reference implementation of CNCF AI patterns, KFabrik provides opinionated defaults based on production best practices. Teams get standard pipeline specifications, common metadata schemas, and tested component compatibility (KServe with Istio/Knative, Ray with KubeRay operators, vLLM with model serving endpoints). Local development environments match CI environments, which mirror production topologies.</p> <p>The project comprises three core components:</p> <ul> <li> <p>kfabric-bootstrap: Bootstraps complete, integrated CNAI stacks with a single command. Handles dependency resolution, version compatibility testing, and configuration for Kubernetes, KServe, Ray, vLLM, vector databases, and supporting infrastructure. Uses tested, locked versions that work together.</p> </li> <li> <p>kfabric-runtime: Orchestrates runtime wiring, health checks, and service discovery across components. Ensures KServe can talk to vector databases, Ray clusters integrate with model serving, and observability captures metrics from all layers. Provides fast feedback when integration breaks.</p> </li> <li> <p>kfabric-control: Provides declarative APIs for stack management and automation. Enables teams to version-control entire AI infrastructure as code, with profiles for different integration patterns (inference-only, RAG, distributed training, etc.).</p> </li> </ul> <p>How It Works</p> <p>Developers run a single command specifying their desired stack profile. KFabrik downloads, configures, and validates all components, bringing up a fully functional CNAI environment ready for model development and testing. The same configuration runs identically in GitHub Actions, GitLab CI, or Jenkins, enabling teams to validate AI workloads before production deployment.</p> <p>\"KFabrik transformed our workflow,\" said an early adopter from a financial services firm. \"We went from three weeks to set up a KServe environment to 15 minutes. Our data scientists now test models locally with the exact same infrastructure our platform runs in production. Integration bugs dropped by 70%.\"</p> <p>Built for the CNCF Ecosystem</p> <p>KFabrik is designed as a potential CNCF project, directly addressing the integration and cohesion gap identified in the CNCF AI landscape. As the CNCF AI whitepaper recommends, \"the community should continue to identify overlaps and consolidate efforts (for example, aligning KServe, Kubeflow Pipelines, and other pieces under compatible versions and interfaces).\" KFabrik does exactly this\u2014providing a unified reference stack that aligns CNCF AI components under tested, compatible configurations.</p> <p>Built on cloud-native principles\u2014declarative configuration, reproducible builds, secure defaults, and zero vendor lock-in\u2014KFabrik leverages existing CNCF projects rather than replacing them. It provides standard pipeline specifications, common metadata schemas, and integration patterns that currently don't exist across the fragmented CNCF AI ecosystem.</p> <p>The project is available now under an open-source license at github.com/kfabrik. Documentation, quickstart guides, and community support are available at kfabrik.io.</p> <p>About KFabrik</p> <p>KFabrik is an open-source project dedicated to solving the integration and cohesion problem in cloud-native AI. By providing unified, tested CNAI stacks, KFabrik makes CNCF AI technologies accessible to every developer. Teams get working integrations of KServe, Ray, vLLM, and vector databases instead of spending weeks on manual compatibility testing and configuration. KFabrik accelerates AI innovation by eliminating the fragmentation tax that currently slows CNAI adoption.</p>"},{"location":"prfaq/#frequently-asked-questions","title":"FREQUENTLY ASKED QUESTIONS","text":""},{"location":"prfaq/#general-questions","title":"General Questions","text":"<p>Q: What is KFabrik?</p> <p>A: KFabrik is an open-source developer and CI tool that provides deterministic, reproducible deployment of complete cloud-native AI (CNAI) stacks. It enables developers to spin up production-like environments containing Kubernetes, KServe, Ray, vLLM, vector databases, and other CNCF AI technologies in minutes rather than weeks. KFabrik is designed specifically for local development and CI/CD testing workflows, not production deployment.</p> <p>Q: Why does this problem need solving?</p> <p>A: The CNCF cloud-native AI ecosystem suffers from a critical integration and cohesion gap. As the CNCF AI whitepaper states: \"Today's AI platforms are a patchwork of specialized tools. This fragmentation means higher complexity and risk. A unified cloud-native AI platform or reference stack could greatly simplify adoption.\"</p> <p>While production-grade technologies like KServe, Ray, and vLLM exist, each project has its own installer, versioning, and configuration patterns. Integrating them requires manual wiring and compatibility testing\u2014work that every team duplicates independently. There are no standard pipeline specifications or common metadata schemas across CNCF AI projects. Teams waste weeks solving version conflicts (does KServe 0.11 work with Istio 1.19?), integration challenges (how do I wire Ray to a vector database?), and environment mismatches.</p> <p>According to CNCF research, \"data scientists develop their ML Python scripts with small datasets locally, and then distributed systems engineers rewrite these scripts for distributed execution\"\u2014a costly, error-prone process. The ecosystem needs what Kubernetes provides for container orchestration: a cohesive base that \"fills the gap\" and creates a unified MLOps experience. KFabrik directly addresses this by providing tested, integrated stacks so teams can focus on AI features instead of infrastructure plumbing.</p> <p>Q: Who is KFabrik for?</p> <p>A: KFabrik targets three primary audiences:</p> <ol> <li> <p>AI/ML Developers and Data Scientists: Those building AI applications who need local environments to develop and test models with KServe, Ray, or vLLM without waiting for shared infrastructure.</p> </li> <li> <p>Platform and DevOps Engineers: Teams responsible for building and maintaining CI/CD pipelines for AI workloads who need reliable, reproducible test environments.</p> </li> <li> <p>Organizations Adopting CNCF AI Technologies: Companies evaluating or migrating to cloud-native AI stacks who need fast proof-of-concept environments and developer onboarding tools.</p> </li> </ol> <p>Q: What problem does KFabrik solve that existing tools don't?</p> <p>A: KFabrik solves the integration and cohesion gap that existing tools don't address:</p> <p>Individual project installers: Each CNCF project (KServe, KubeRay, vector databases) has its own installer, but they don't provide tested integration. You can install KServe and Ray independently, but wiring them together\u2014ensuring compatible versions, configuring networking, setting up service discovery\u2014requires weeks of manual work and tribal knowledge.</p> <p>Manual integration approaches: Organizations spend engineering cycles solving the same problems: \"Which KServe version works with Istio 1.19?\" \"How do I configure Ray to work with this vector database?\" \"Why does vLLM work locally but fail in CI?\" Every team duplicates this integration effort independently. There's no standard reference stack.</p> <p>Kubeflow: Broader scope focused on the full ML lifecycle (training pipelines, hyperparameter tuning, notebooks). While comprehensive, it's complex to set up and heavier than needed for teams focused on inference/serving workflows. Kubeflow itself acknowledges the complexity problem.</p> <p>Cloud provider managed services: AWS SageMaker, Google Vertex AI provide integrated experiences but create vendor lock-in, don't support local development, and use proprietary APIs that differ from open CNCF standards.</p> <p>What's missing: A unified, tested reference implementation that integrates CNCF AI components with compatible versions and standard configurations. As the CNCF AI whitepaper notes, \"a unified cloud-native AI platform or reference stack could greatly simplify adoption\" and \"the community should continue to identify overlaps and consolidate efforts (for example, aligning KServe, Kubeflow Pipelines, and other pieces under compatible versions and interfaces).\"</p> <p>KFabrik fills this gap by providing opinionated, pre-integrated CNAI stacks purpose-built for developer and CI workflows. Instead of artisanal integration work, teams get tested, locked configurations that work across dev, CI, and prod-like environments.</p>"},{"location":"prfaq/#customer-questions","title":"Customer Questions","text":"<p>Q: How long does it take to get started with KFabrik?</p> <p>A: A developer with Docker installed can have a complete KServe-based inference environment running locally in under 15 minutes. This includes Kubernetes (kind or k3s), KServe, Istio/Knative, and a sample model deployment. More complex stacks including Ray clusters and vector databases take 20-30 minutes depending on hardware and network speed. Compare this to the typical 2-4 weeks required for manual setup and integration testing.</p> <p>Q: What if my production environment uses different versions or configurations?</p> <p>A: KFabrik uses declarative configuration files that specify exact component versions, resource allocations, and integration settings. Teams can maintain multiple profiles (e.g., \"dev-minimal\", \"ci-integration\", \"prod-mirror\") and share them via version control. When production configurations change, updating the KFabrik profile ensures all developers and CI pipelines stay synchronized. This \"infrastructure as code\" approach eliminates environment drift.</p> <p>Q: Can I customize the stack for my needs?</p> <p>A: Yes. KFabrik provides opinionated defaults that work out-of-the-box, but every aspect is customizable through configuration files. You can:</p> <ul> <li>Select specific versions of Kubernetes, KServe, Ray, etc.</li> <li>Add or remove components (e.g., skip Ray if you only need KServe)</li> <li>Configure resource limits, networking, and storage</li> <li>Integrate custom operators or CRDs</li> <li>Add organization-specific security policies or secrets management</li> </ul> <p>The philosophy is \"opinionated defaults, unlimited flexibility.\"</p> <p>Q: Does this work in air-gapped or regulated environments?</p> <p>A: Yes. KFabrik is designed with air-gapped and regulated environments in mind. The bootstrap component supports:</p> <ul> <li>Pre-cached container images and artifacts</li> <li>Custom registry configuration</li> <li>Offline installation bundles</li> <li>Compliance-focused profiles with security hardening</li> </ul> <p>This makes KFabrik suitable for financial services, healthcare, government, and other regulated industries.</p> <p>Q: What's the learning curve?</p> <p>A: KFabrik significantly reduces the learning curve for CNAI adoption. Instead of mastering Kubernetes, Istio, Knative, KServe, Ray, and various operators independently, developers can start with a working stack and learn components incrementally. Documentation includes:</p> <ul> <li>Quick-start guides for common use cases</li> <li>Concept explanations for each component</li> <li>Troubleshooting runbooks</li> <li>Migration guides from manual setups</li> </ul> <p>Early adopters report that junior developers become productive with KServe-based development in days instead of weeks.</p> <p>Q: How does KFabrik help with CI/CD?</p> <p>A: KFabrik integrates directly into CI/CD pipelines (GitHub Actions, GitLab CI, Jenkins, etc.) by providing:</p> <ul> <li>Containerized environments that start quickly in CI runners</li> <li>Deterministic, cached builds for fast pipeline execution</li> <li>Automated health checks to validate stack readiness</li> <li>Test fixtures for common AI workload patterns</li> </ul> <p>Teams can run integration tests against real KServe endpoints, validate Ray job execution, or test vector database queries\u2014all within their CI pipeline using the same configuration as local development.</p>"},{"location":"prfaq/#technical-questions","title":"Technical Questions","text":"<p>Q: What technologies does KFabrik include?</p> <p>A: KFabrik provides integrated stacks built from CNCF and cloud-native ecosystem projects, including:</p> <ul> <li>Kubernetes: Kind, k3s, or k3d for local clusters</li> <li>KServe: Model serving platform with inference protocols</li> <li>Istio/Knative: Service mesh and serverless components required by KServe</li> <li>Ray and KubeRay: Distributed computing framework for AI workloads</li> <li>vLLM: High-performance LLM inference engine</li> <li>Vector databases: Milvus, Qdrant, or others for RAG patterns</li> <li>Storage: MinIO or local storage providers</li> <li>Observability: Optional Prometheus, Grafana, and Jaeger integration</li> </ul> <p>Component selection is configurable per stack profile.</p> <p>Q: What are the system requirements?</p> <p>A: Minimum requirements for basic KServe development:</p> <ul> <li>CPU: 4 cores (8 recommended)</li> <li>RAM: 8GB (16GB recommended)</li> <li>Disk: 20GB free space</li> <li>OS: Linux, macOS, or Windows with WSL2</li> <li>Software: Docker or Podman</li> </ul> <p>For Ray clusters or LLM serving, requirements increase:</p> <ul> <li>CPU: 8+ cores</li> <li>RAM: 32GB+ (especially for vLLM with local models)</li> <li>GPU: Optional but recommended for LLM inference testing</li> </ul> <p>CI environments typically use 8-core runners with 16-32GB RAM.</p> <p>Q: How does KFabrik handle component integration and compatibility?</p> <p>A: KFabrik solves the integration problem through tested, locked configurations. Rather than hoping components work together, KFabrik provides:</p> <p>Version Lock Files: Every stack profile specifies exact versions for all components (e.g., KServe 0.11.2, Istio 1.19.3, Ray 2.9.0). These combinations are tested together in CI to ensure compatibility. When KServe releases a new version, KFabrik maintainers test it against current Istio/Knative versions before updating lock files.</p> <p>Integration Testing: Automated tests validate that components wire together correctly\u2014KServe can serve models, Ray clusters accept jobs, vector databases respond to queries, observability captures metrics from all layers. This catches integration breaks before users encounter them.</p> <p>Standard Configuration Patterns: KFabrik provides tested configurations for common patterns: - KServe + Istio/Knative: Correct service mesh setup, ingress configuration, autoscaling policies - Ray + KubeRay: Proper operator installation, cluster CRDs, distributed computing setup - vLLM + Model Serving: GPU allocation, model weight management, inference endpoint configuration - Vector DB integration: Networking setup, persistent storage, query endpoint exposure</p> <p>Runtime Wiring: The kfabric-runtime component handles cross-component integration\u2014ensuring KServe inference graphs can call vector databases for RAG, Ray jobs can access model registries, observability dashboards show metrics from all services.</p> <p>This eliminates the \"artisanal integration\" problem where every team figures out compatibility independently. Teams get tested, working stacks and can focus on AI features instead of infrastructure debugging.</p> <p>Q: How does KFabrik ensure reproducibility?</p> <p>A: KFabrik achieves reproducibility through:</p> <ol> <li>Version pinning: Every component version is specified in lock files, similar to package managers like npm or cargo.</li> <li>Declarative configuration: Stack definitions are pure data, producing identical results on every run.</li> <li>Idempotent operations: Running bootstrap multiple times produces the same result; no hidden state.</li> <li>Content-addressable artifacts: Container images and binaries are verified by checksum.</li> <li>Hermetic builds: All dependencies are explicit; no reliance on system state.</li> </ol> <p>This ensures that \"it works on my machine\" actually means \"it will work everywhere.\"</p> <p>Q: How does KFabrik handle updates and upgrades?</p> <p>A: KFabrik separates stack definition from installation:</p> <ul> <li>Stack profiles define desired component versions</li> <li>Bootstrap realizes the profile in a target environment</li> <li>Updates are performed by modifying the profile and re-running bootstrap</li> </ul> <p>This approach enables:</p> <ul> <li>Testing updates in isolated environments before rolling out</li> <li>Rolling back to previous configurations via version control</li> <li>Progressive rollouts (update dev, then CI, then prod-mirror)</li> </ul> <p>Component-level updates (e.g., upgrading just KServe) are supported without tearing down the entire stack.</p> <p>Q: What about production deployment?</p> <p>A: KFabrik is explicitly scoped for development and CI workflows, not production deployment. However, the stack configurations KFabrik uses can inform production deployments via GitOps tools like Flux or ArgoCD. Teams typically:</p> <ol> <li>Use KFabrik for local development and CI testing</li> <li>Extract validated configurations as Helm charts or Kustomize overlays</li> <li>Deploy to production using enterprise-grade GitOps tooling</li> </ol> <p>This separation of concerns ensures KFabrik stays focused on developer productivity while production deployments use appropriate tooling for scale, multi-tenancy, and operational requirements.</p> <p>Q: How does this integrate with existing Kubernetes clusters?</p> <p>A: KFabrik can work in two modes:</p> <ol> <li>Standalone: Creates a new local Kubernetes cluster (kind/k3s) with the CNAI stack</li> <li>Cluster mode: Installs CNAI components into an existing Kubernetes cluster</li> </ol> <p>Cluster mode is useful for shared development clusters or testing against specific Kubernetes distributions. Safety checks prevent accidental installation on production clusters.</p>"},{"location":"prfaq/#business-and-strategic-questions","title":"Business and Strategic Questions","text":"<p>Q: What's the licensing model?</p> <p>A: KFabrik is fully open-source under the Apache 2.0 license, consistent with CNCF project requirements. There are no proprietary components, no commercial versions, and no vendor lock-in. Organizations can use, modify, and distribute KFabrik freely.</p> <p>Q: Why are you pursuing CNCF project status?</p> <p>A: CNCF provides the ideal home for KFabrik because:</p> <ol> <li> <p>Addressing a CNCF-identified gap: The CNCF AI whitepaper explicitly identifies the integration and cohesion problem\u2014\"Today's AI platforms are a patchwork of specialized tools\" and recommends \"a unified cloud-native AI platform or reference stack.\" KFabrik directly addresses this need.</p> </li> <li> <p>Alignment: KFabrik exists to integrate and unify CNCF AI projects (KServe, KubeRay, vector databases), making them accessible as a cohesive stack rather than isolated tools.</p> </li> <li> <p>Neutrality: CNCF's vendor-neutral governance prevents any single company from controlling integration standards. A neutral integration layer is critical for ecosystem adoption.</p> </li> <li> <p>Cross-project coordination: As a CNCF project, KFabrik can work directly with KServe, Kubeflow, and other CNCF AI projects to \"identify overlaps and consolidate efforts\" under compatible versions and interfaces.</p> </li> <li> <p>Sustainability: CNCF provides infrastructure, support, and community that ensure long-term viability of integration testing and compatibility matrices.</p> </li> <li> <p>Standards development: CNCF technical oversight helps KFabrik establish standard pipeline specifications and common metadata schemas that the ecosystem currently lacks.</p> </li> </ol> <p>KFabrik complements existing CNCF AI projects by solving the \"developer experience and integration gap\"\u2014the missing cohesive layer that makes cloud-native AI practical for everyday developers.</p> <p>Q: How does KFabrik make money?</p> <p>A: KFabrik is a community-driven open-source project, not a commercial product. Contributors participate because CNAI complexity is a shared industry problem, and a neutral, open solution benefits everyone. Potential business models around KFabrik could include:</p> <ul> <li>Hosted services for enterprise CI/CD</li> <li>Support and consulting services</li> <li>Training and certification programs</li> <li>Managed cloud instances</li> </ul> <p>However, the core project remains free and open-source.</p> <p>Q: What's the roadmap?</p> <p>A: The initial release focuses on KServe-centric workflows with Ray and vLLM integration. Future plans include:</p> <ul> <li>Q1: Enhanced vector database support, RAG pattern templates</li> <li>Q2: Kubeflow integration for training workflows, multi-cluster support</li> <li>Q3: Advanced observability, cost estimation tools, security scanning</li> <li>Q4: Windows native support, ARM architecture support, performance optimizations</li> </ul> <p>Community feedback will heavily influence prioritization.</p> <p>Q: How can we contribute or get involved?</p> <p>A: KFabrik welcomes contributions:</p> <ul> <li>Code: Submit PRs for features, bug fixes, or documentation</li> <li>Testing: Try KFabrik with your workloads and report issues</li> <li>Documentation: Improve guides, add examples, translate content</li> <li>Community: Answer questions, help other users, present at meetups</li> </ul> <p>Visit github.com/kfabrik/community for contribution guidelines, development setup, and communication channels (Slack, mailing list, monthly meetings).</p> <p>Q: What's different about KFabrik's approach to \"opinionated defaults\"?</p> <p>A: Many tools claim to provide opinionated defaults but actually shift complexity rather than eliminating it. KFabrik's philosophy:</p> <ol> <li>Boring reliability over cutting edge: Default to stable, well-tested versions</li> <li>Production patterns in dev: Defaults mirror production topologies (e.g., Istio + Knative for KServe, even locally)</li> <li>Secure by default: Enable authentication, TLS, and RBAC out-of-the-box</li> <li>Escape hatches everywhere: Every default can be overridden; no magic</li> </ol> <p>This balances \"just works\" for beginners with \"full control\" for advanced users.</p>"},{"location":"prfaq/#competitive-questions","title":"Competitive Questions","text":"<p>Q: How does KFabrik compare to Kubeflow?</p> <p>A: Kubeflow and KFabrik serve different needs but can be complementary:</p> <p>Kubeflow is a comprehensive ML platform covering the entire ML lifecycle (data prep, training, hyperparameter tuning, serving, pipelines). It's broad, powerful, and addresses production ML workflows end-to-end. However, Kubeflow's comprehensiveness comes with complexity\u2014it's notoriously difficult to install and configure, often taking weeks to get operational. As one CNCF TOC member noted, Kubeflow is needed to \"fill the gap\" and create a cohesive MLOps ecosystem, but integration challenges remain.</p> <p>KFabrik focuses narrowly on solving the developer and CI environment problem. It provides pre-integrated, tested stacks for cloud-native AI inference/serving workloads (KServe, Ray, vLLM, vector databases) that work out-of-the-box. KFabrik is lightweight, installs in minutes, and optimized for rapid iteration in dev/test workflows. It doesn't attempt to cover the full ML lifecycle\u2014just the integration and cohesion problem for CNAI components.</p> <p>When to use each: - Use Kubeflow when you need full ML lifecycle management in production (training pipelines, experiment tracking, model registry) - Use KFabrik when you need fast, reproducible dev/test environments with integrated CNAI components - Use both: KFabrik for local development and CI testing, Kubeflow for production ML pipelines and lifecycle management</p> <p>KFabrik can actually make Kubeflow adoption easier by providing a quick dev environment where developers can experiment with Kubeflow Pipelines or KServe integration before committing to a full production Kubeflow deployment.</p> <p>Q: Why not just use Docker Compose?</p> <p>A: Docker Compose works well for simple multi-container applications but breaks down for cloud-native AI:</p> <ol> <li>No Kubernetes: KServe, Ray, and most CNAI tools require Kubernetes primitives (CRDs, operators, RBAC)</li> <li>No resource management: GPU allocation, autoscaling, and distributed workloads need Kubernetes schedulers</li> <li>Deployment gap: Compose configs don't translate to production Kubernetes deployments</li> </ol> <p>KFabrik uses real Kubernetes, ensuring dev environments match production architecture.</p> <p>Q: What about cloud provider AI services (AWS SageMaker, Google Vertex AI, Azure ML)?</p> <p>A: Managed AI services are excellent for production workloads but create challenges for development:</p> <ul> <li>Vendor lock-in: APIs differ across providers; migrating is costly</li> <li>Cost: Running persistent dev environments in the cloud is expensive</li> <li>Latency: Slow iteration cycles when testing requires cloud deployment</li> <li>Connectivity: Requires internet access; difficult in air-gapped scenarios</li> </ul> <p>KFabrik provides local, CNCF-native environments that work identically across clouds or on-prem. Teams can develop locally with KFabrik and deploy to any Kubernetes, including managed services that support CNCF standards.</p> <p>Q: Couldn't we build this ourselves?</p> <p>A: Yes\u2014and many teams do. The problem is that every team solves the same integration challenges independently, wasting engineering effort. KFabrik consolidates this duplicated work into a shared, tested, community-maintained solution. Building internally means:</p> <ul> <li>Ongoing maintenance burden as CNCF projects evolve</li> <li>Knowledge concentrated in a few engineers (bus factor risk)</li> <li>No community to share improvements or fixes</li> </ul> <p>Using KFabrik lets teams focus on AI features, not infrastructure plumbing.</p> <p>Document Version: 1.0 Last Updated: 2025-12-17 Status: Draft for Community Review</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"KFabrik Documentation","text":"<p>Welcome to KFabrik, an integrated platform for deploying and managing Large Language Models (LLMs) on local Kubernetes clusters.</p>"},{"location":"#what-is-kfabrik","title":"What is KFabrik?","text":"<p>KFabrik enables ML developers to deploy LLM inference servers on minikube for local development and testing with a single command. A typical deployment takes under 10 minutes and requires no manual configuration of Kubernetes resources, service mesh routing, or monitoring infrastructure.</p> <p>Note: KFabrik is designed for development and testing purposes only. It is not intended for production deployments.</p>"},{"location":"#the-problem-we-solve","title":"The Problem We Solve","text":"<p>ML developers face significant friction when testing LLM deployments locally:</p> <ul> <li>Configuration Burden: Deploying a single model requires creating and maintaining dozens of Kubernetes manifests across multiple namespaces</li> <li>Dependency Management: The ML serving stack has strict ordering requirements that frequently fail due to race conditions</li> <li>Observability Gap: Understanding model performance requires correlating inference latency metrics with GPU utilization</li> <li>Reproducibility: Development environments vary between team members, making it hard to share and reproduce issues</li> </ul> <p>KFabrik solves these problems by providing an opinionated, fully-integrated ML inference platform that deploys with a single command.</p>"},{"location":"#core-components","title":"Core Components","text":"<p>KFabrik comprises a CLI and three minikube addons:</p>"},{"location":"#kfabrik-cli","title":"kfabrik CLI","text":"<p>Command-line interface for model deployment, querying, and management. Provides commands for:</p> <ul> <li>Starting/stopping clusters with GPU support</li> <li>Deploying and managing models</li> <li>Querying models via OpenAI-compatible API</li> <li>Viewing logs and status</li> </ul>"},{"location":"#kfabrik-bootstrap-addon","title":"kfabrik-bootstrap Addon","text":"<p>Installs foundational infrastructure:</p> Component Purpose Cert-Manager TLS certificate management Istio Service mesh and ingress routing KServe Model serving platform NVIDIA Device Plugin GPU resource scheduling"},{"location":"#kfabrik-model-addon","title":"kfabrik-model Addon","text":"<p>Provides pre-configured model definitions optimized for consumer GPUs (6GB VRAM):</p> Model Parameters VRAM Description qwen-small 0.5B ~1GB Qwen 2.5 0.5B Instruct qwen-medium 1.5B ~3GB Qwen 2.5 1.5B Instruct tinyllama 1.1B ~2.5GB TinyLlama 1.1B Chat smollm2 1.7B ~3.5GB SmolLM2 1.7B Instruct phi2 2.7B ~5.5GB Microsoft Phi-2"},{"location":"#kfabrik-monitoring-addon","title":"kfabrik-monitoring Addon","text":"<p>Deploys observability stack:</p> Component Purpose Prometheus Metrics collection Grafana Visualization DCGM Exporter GPU metrics"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone the custom minikube repository\ngit clone https://github.com/kfabrik/minikube.git\ncd minikube\n\n# Build minikube with kfabrik addons\nmake build\n\n# Install kfabrik CLI and minikube\n./scripts/install.sh\n\n# Start cluster with GPU support\nkfabrik cluster start\n\n# List available models\nkfabrik list\n\n# Deploy a model\nkfabrik deploy --models qwen-small --wait\n\n# Query the model\nkfabrik query --model qwen-small --prompt \"What is Kubernetes?\"\n\n# Clean up\nkfabrik delete --model qwen-small\nkfabrik cluster stop\n</code></pre>"},{"location":"#design-principles","title":"Design Principles","text":"<p>Simplicity over flexibility: Optimized for the common case of deploying HuggingFace models on GPU-enabled minikube clusters.</p> <p>Consistent environments: All developers get identical local configurations using standard KServe InferenceService specifications and Istio routing rules.</p> <p>Explicit over implicit: All configuration is visible and auditable. Standard Kubernetes resources that can be inspected, modified, and versioned.</p> <p>Fast feedback: Deployment speed optimized through parallel installations, aggressive health checks, and clear progress reporting.</p> <p>GPU-first: Assumes GPU workloads are the primary use case. Resource defaults are tuned for NVIDIA GPUs with 6GB+ VRAM.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Installation and setup guide</li> <li>CLI Reference - Complete command documentation</li> <li>Architecture - Detailed design documentation</li> <li>Addons - Addon configuration and troubleshooting</li> <li>Contributing - How to contribute to the project</li> </ul>"},{"location":"addons/","title":"Addons","text":"<p>The KFabrik addon suite provides a complete machine learning inference platform for minikube, enabling local development and testing of Large Language Models (LLMs) with GPU support.</p>"},{"location":"addons/#overview","title":"Overview","text":"<p>The suite consists of three addons that work together:</p> <pre><code>flowchart LR\n    bootstrap[\"kfabrik-bootstrap&lt;br/&gt;(required first)\"]\n    model[\"kfabrik-model&lt;br/&gt;(optional)\"]\n    monitoring[\"kfabrik-monitoring&lt;br/&gt;(optional)\"]\n\n    bootstrap --&gt; model\n    bootstrap --&gt; monitoring</code></pre> <p>The kfabrik-bootstrap addon must be enabled first as it provides the core infrastructure (KServe, Istio) that other addons depend on.</p>"},{"location":"addons/#kfabrik-bootstrap","title":"kfabrik-bootstrap","text":""},{"location":"addons/#purpose","title":"Purpose","text":"<p>The foundation addon that installs all core infrastructure components required for ML inference workloads.</p>"},{"location":"addons/#components","title":"Components","text":"Component Purpose Cert Manager TLS certificate lifecycle management Istio Service mesh for traffic and security KServe Kubernetes-native ML model serving NVIDIA Device Plugin GPU resource discovery and allocation"},{"location":"addons/#installation","title":"Installation","text":"<pre><code># Enable the addon\nminikube addons enable kfabrik-bootstrap\n\n# Monitor installation progress\nkubectl get jobs -n kserve -w\n\n# Verify components are running\nkubectl get pods -n cert-manager\nkubectl get pods -n istio-system\nkubectl get pods -n kserve\nkubectl get pods -n kube-system | grep nvidia\n</code></pre>"},{"location":"addons/#namespaces-created","title":"Namespaces Created","text":"Namespace Contents cert-manager Certificate management components and webhooks istio-system Istio control plane (istiod) kserve KServe controller and installer job"},{"location":"addons/#configuration","title":"Configuration","text":"<p>Configuration is stored in a ConfigMap in the kserve namespace:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kfabrik-bootstrap-config\n  namespace: kserve\ndata:\n  INSTALL_CERT_MANAGER: \"true\"\n  INSTALL_ISTIO: \"true\"\n  INSTALL_KSERVE: \"true\"\n</code></pre>"},{"location":"addons/#design-decisions","title":"Design Decisions","text":"<p>Helm-Based Installation: Uses a Job-based installer that runs Helm charts rather than static YAML manifests. This provides version pinning, configuration management, and upgrade capabilities.</p> <p>RawDeployment Mode: KServe is configured for RawDeployment mode (standard Kubernetes Deployments) rather than Serverless mode (Knative). This simplifies setup, debugging, and operation for local development. Knative support is planned for a future release.</p> <p>Job-Based Installer: A Kubernetes Job orchestrates sequential installation of components with proper dependency ordering. The job self-cleans after 5 minutes (TTL).</p>"},{"location":"addons/#troubleshooting","title":"Troubleshooting","text":"<p>Check installer job: <pre><code>kubectl get jobs -n kserve\nkubectl logs -n kserve -l job-name=kfabrik-installer\n</code></pre></p> <p>Verify GPU detection: <pre><code>kubectl get nodes -o json | jq '.items[].status.capacity'\nkubectl logs -n kube-system -l name=nvidia-device-plugin-ds\n</code></pre></p>"},{"location":"addons/#kfabrik-model","title":"kfabrik-model","text":""},{"location":"addons/#purpose_1","title":"Purpose","text":"<p>Provides model deployment configurations and the model-serving namespace for running LLM inference workloads.</p>"},{"location":"addons/#pre-configured-models","title":"Pre-Configured Models","text":"<p>All models are optimized for consumer GPUs with 6GB VRAM or less:</p> Name Parameters VRAM RAM Download qwen-small 0.5B ~1GB ~6GB ~1GB qwen-medium 1.5B ~3GB ~8GB ~3GB tinyllama 1.1B ~2.5GB ~6GB ~2.2GB smollm2 1.7B ~3.5GB ~8GB ~3.4GB phi2 2.7B ~5.5GB ~12GB ~5.5GB <p>RAM requirements are approximately 2-3x model size due to inference server overhead. Models are deployed one at a time.</p>"},{"location":"addons/#installation_1","title":"Installation","text":"<pre><code># Requires kfabrik-bootstrap to be enabled first\nminikube addons enable kfabrik-bootstrap\n\n# Wait for bootstrap to complete\nkubectl wait --for=condition=Ready pod -l app=istiod \\\n  -n istio-system --timeout=300s\n\n# Enable kfabrik-model\nminikube addons enable kfabrik-model\n</code></pre>"},{"location":"addons/#namespace-created","title":"Namespace Created","text":"Namespace Contents model-serving Deployed InferenceServices and model configuration ConfigMap"},{"location":"addons/#model-configuration","title":"Model Configuration","text":"<p>Model definitions are stored in a ConfigMap:</p> <pre><code>kubectl get configmap model-config -n model-serving -o yaml\n</code></pre> <p>Example model configuration:</p> <pre><code>models:\n  qwen-small:\n    name: qwen-small\n    displayName: \"Qwen 2.5 0.5B Instruct\"\n    modelFormat: huggingface\n    storageUri: \"hf://Qwen/Qwen2.5-0.5B-Instruct\"\n    vram: \"1GB\"\n    ram: \"6GB\"\n    downloadSize: \"1GB\"\n    parameters: \"0.5B\"\n    replicas: 1\n    timeout: 300\n    resources:\n      requests:\n        cpu: \"1\"\n        memory: \"4Gi\"\n      limits:\n        cpu: \"4\"\n        memory: \"8Gi\"\n    env:\n      HF_MODEL_ID: \"Qwen/Qwen2.5-0.5B-Instruct\"\n      MAX_MODEL_LEN: \"512\"\n      GPU_MEMORY_UTILIZATION: \"0.8\"\n</code></pre>"},{"location":"addons/#using-with-kfabrik-cli","title":"Using with kfabrik CLI","text":"<pre><code># List available models\nkfabrik list\n\n# Deploy a model\nkfabrik deploy --models qwen-small --wait\n\n# Query the model\nkfabrik query --model qwen-small --prompt \"What is AI?\"\n\n# Delete the model\nkfabrik delete --model qwen-small\n</code></pre>"},{"location":"addons/#custom-models","title":"Custom Models","text":"<p>To add custom models, create a custom configuration file:</p> <pre><code># my-models.yaml\nnamespace: model-serving\nmodels:\n  my-model:\n    name: my-model\n    displayName: \"My Custom Model\"\n    modelFormat: huggingface\n    storageUri: \"hf://organization/model-name\"\n    resources:\n      requests:\n        cpu: \"2\"\n        memory: \"8Gi\"\n      limits:\n        cpu: \"4\"\n        memory: \"16Gi\"\n</code></pre> <p>Then deploy with: <pre><code>kfabrik deploy --config my-models.yaml --models my-model\n</code></pre></p>"},{"location":"addons/#kfabrik-monitoring","title":"kfabrik-monitoring","text":""},{"location":"addons/#purpose_2","title":"Purpose","text":"<p>Provides a complete observability stack for monitoring ML inference workloads, including GPU metrics.</p>"},{"location":"addons/#components_1","title":"Components","text":"Component Purpose Prometheus Metrics collection and storage Grafana Visualization and dashboards DCGM Exporter NVIDIA GPU metrics exporter"},{"location":"addons/#installation_2","title":"Installation","text":"<pre><code># Enable after kfabrik-bootstrap\nminikube addons enable kfabrik-monitoring\n\n# Verify components\nkubectl get pods -n monitoring\n</code></pre>"},{"location":"addons/#namespace-created_1","title":"Namespace Created","text":"Namespace Contents monitoring Prometheus, Grafana, and DCGM Exporter"},{"location":"addons/#configuration_1","title":"Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kfabrik-monitoring-config\n  namespace: monitoring\ndata:\n  PROMETHEUS_RETENTION: \"15d\"\n  PROMETHEUS_STORAGE_SIZE: \"10Gi\"\n  GRAFANA_ADMIN_USER: \"admin\"\n  GRAFANA_ADMIN_PASSWORD: \"admin\"\n</code></pre>"},{"location":"addons/#accessing-dashboards","title":"Accessing Dashboards","text":"<p>Grafana: <pre><code>kubectl port-forward -n monitoring svc/grafana 3000:3000\n# Open http://localhost:3000\n# Login: admin / admin\n</code></pre></p> <p>Prometheus: <pre><code>kubectl port-forward -n monitoring svc/prometheus 9090:9090\n# Open http://localhost:9090\n</code></pre></p>"},{"location":"addons/#gpu-metrics","title":"GPU Metrics","text":"<p>The DCGM Exporter provides detailed GPU metrics:</p> Metric Description DCGM_FI_DEV_GPU_UTIL GPU utilization percentage DCGM_FI_DEV_MEM_COPY_UTIL Memory copy utilization DCGM_FI_DEV_FB_USED Framebuffer memory used (bytes) DCGM_FI_DEV_FB_FREE Framebuffer memory free (bytes) DCGM_FI_DEV_GPU_TEMP GPU temperature (Celsius) DCGM_FI_DEV_POWER_USAGE Power consumption (Watts)"},{"location":"addons/#resource-requirements","title":"Resource Requirements","text":""},{"location":"addons/#minimum-requirements","title":"Minimum Requirements","text":"Resource Minimum CPU 4 cores Memory 8GB RAM Disk 40GB"},{"location":"addons/#recommended-for-gpu-workloads","title":"Recommended for GPU Workloads","text":"Resource Recommended CPU 8+ cores Memory 16-32GB RAM Disk 50GB+ GPU VRAM 6GB+"},{"location":"addons/#starting-minikube-with-appropriate-resources","title":"Starting minikube with Appropriate Resources","text":"<pre><code># Set persistent defaults\nminikube config set cpus 8\nminikube config set memory 32768\nminikube config set disk-size 50g\n\n# Start with GPU support\nminikube start --driver=docker --gpus=all\n</code></pre>"},{"location":"addons/#disabling-addons","title":"Disabling Addons","text":"<pre><code># Disable in reverse order of dependencies\nminikube addons disable kfabrik-monitoring\nminikube addons disable kfabrik-model\nminikube addons disable kfabrik-bootstrap\n</code></pre> <p>For complete cleanup including CRDs:</p> <pre><code># Delete Helm releases\nhelm uninstall kserve -n kserve\nhelm uninstall istiod -n istio-system\nhelm uninstall istio-base -n istio-system\nhelm uninstall cert-manager -n cert-manager\n\n# Delete namespaces\nkubectl delete namespace model-serving monitoring \\\n  kserve istio-system cert-manager\n\n# Delete CRDs\nkubectl get crd | grep -E 'kserve|istio|cert-manager' | \\\n  awk '{print $1}' | xargs kubectl delete crd\n</code></pre>"},{"location":"addons/#troubleshooting_1","title":"Troubleshooting","text":""},{"location":"addons/#addon-enable-fails","title":"Addon Enable Fails","text":"<pre><code># Check addon status\nminikube addons list | grep kfabrik\n\n# View installer logs\nkubectl logs -n kserve -l job-name=kfabrik-installer\n\n# Check events\nkubectl get events -n kserve --sort-by='.lastTimestamp'\n</code></pre>"},{"location":"addons/#models-not-deploying","title":"Models Not Deploying","text":"<pre><code># Check InferenceService status\nkubectl get inferenceservice -n model-serving\n\n# Describe for detailed conditions\nkubectl describe inferenceservice &lt;name&gt; -n model-serving\n\n# Check pod status\nkubectl get pods -n model-serving\nkubectl logs -n model-serving &lt;pod-name&gt;\n</code></pre>"},{"location":"addons/#gpu-not-detected","title":"GPU Not Detected","text":"<pre><code># Verify device plugin is running\nkubectl get pods -n kube-system | grep nvidia\n\n# Check plugin logs\nkubectl logs -n kube-system -l name=nvidia-device-plugin-ds\n\n# Verify GPU is advertised\nkubectl describe node minikube | grep nvidia.com/gpu\n</code></pre>"},{"location":"addons/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Increase minikube memory allocation: <pre><code>minikube stop\nminikube config set memory 32768\nminikube start --gpus=all\n</code></pre></p>"},{"location":"addons/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - kfabrik command documentation</li> <li>Architecture - Detailed design documentation</li> <li>Getting Started - Installation guide</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This document describes the architecture, component interactions, and operational characteristics of the KFabrik platform.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>KFabrik consists of four primary components that work together to provide a complete ML inference platform:</p> <pre><code>flowchart TB\n    subgraph Workstation[\"User Workstation\"]\n        CLI[\"kfabrik CLI\"]\n    end\n\n    CLI --&gt;|\"kubectl / Kubernetes API\"| Cluster\n\n    subgraph Cluster[\"Minikube Cluster (Docker Driver)\"]\n        subgraph Bootstrap[\"kfabrik-bootstrap addon\"]\n            CertManager[\"Cert-Manager\"]\n            Istio[\"Istio\"]\n            KServe[\"KServe\"]\n            NvidiaPlugin[\"NVIDIA Device Plugin\"]\n        end\n\n        subgraph Model[\"kfabrik-model addon\"]\n            subgraph ModelServing[\"model-serving namespace\"]\n                ConfigMap[\"ConfigMap: model-config\"]\n                InferenceServices[\"InferenceServices\"]\n                Predictors[\"Predictor Pods\"]\n            end\n        end\n\n        subgraph Monitoring[\"kfabrik-monitoring addon\"]\n            Prometheus[\"Prometheus\"]\n            Grafana[\"Grafana\"]\n            DCGM[\"DCGM Exporter\"]\n        end\n    end</code></pre>"},{"location":"architecture/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"architecture/#kfabrik-cli","title":"kfabrik CLI","text":"<p>Provides a command-line interface for deploying models, querying inference endpoints, and managing the model lifecycle. The CLI communicates with the Kubernetes API to create InferenceService resources and uses kubectl port-forwarding to access inference endpoints.</p> <p>Kubernetes Client Architecture:</p> <ul> <li>Typed Clientset (<code>kubernetes.Clientset</code>) for standard Kubernetes resources (ConfigMaps, Pods, Services)</li> <li>Dynamic Client (<code>dynamic.Interface</code>) for custom resources (InferenceServices)</li> </ul>"},{"location":"architecture/#kfabrik-bootstrap-addon","title":"kfabrik-bootstrap Addon","text":"<p>Installs the foundational infrastructure required for model serving:</p> <ul> <li>Cert-Manager for TLS certificate management</li> <li>Istio for service mesh routing</li> <li>KServe for model lifecycle management</li> <li>NVIDIA Device Plugin for GPU resource scheduling</li> </ul> <p>The addon uses a Kubernetes Job to orchestrate Helm-based installations in dependency order.</p>"},{"location":"architecture/#kfabrik-model-addon","title":"kfabrik-model Addon","text":"<p>Creates the model-serving namespace and deploys a ConfigMap containing pre-configured model definitions. These definitions specify HuggingFace model URIs, resource requirements, and inference server parameters optimized for 6GB VRAM GPUs.</p>"},{"location":"architecture/#kfabrik-monitoring-addon","title":"kfabrik-monitoring Addon","text":"<p>Deploys the observability stack:</p> <ul> <li>Prometheus for metrics collection</li> <li>Grafana for visualization</li> <li>DCGM Exporter for GPU-specific metrics</li> </ul> <p>Prometheus is pre-configured to scrape KServe inference metrics and DCGM GPU metrics.</p>"},{"location":"architecture/#bootstrap-installation-sequence","title":"Bootstrap Installation Sequence","text":"<p>The bootstrap addon installs components through a sequenced process with explicit health checks:</p> <pre><code>flowchart TD\n    subgraph Phase1[\"Phase 1: Cert-Manager Installation\"]\n        P1A[\"Add Jetstack Helm repository\"] --&gt; P1B[\"Helm install cert-manager with CRDs\"]\n        P1B --&gt; P1C[\"Wait for deployments\"]\n        P1C --&gt; P1D[\"Verify CRD exists\"]\n    end\n\n    subgraph Phase2[\"Phase 2: Istio Installation\"]\n        P2A[\"Add Istio Helm repository\"] --&gt; P2B[\"Helm install istio-base\"]\n        P2B --&gt; P2C[\"Helm install istiod\"]\n        P2C --&gt; P2D[\"Wait for istiod deployment\"]\n        P2D --&gt; P2E[\"Create IngressClass\"]\n    end\n\n    subgraph Phase3[\"Phase 3: KServe Cleanup\"]\n        P3A[\"Delete existing ClusterRoles\"] --&gt; P3B[\"Delete webhooks &amp; CRDs\"]\n        P3B --&gt; P3C[\"Wait for cleanup\"]\n    end\n\n    subgraph Phase4[\"Phase 4: KServe Installation\"]\n        P4A[\"Helm install kserve-crd\"] --&gt; P4B[\"Helm install kserve controller\"]\n        P4B --&gt; P4C[\"Wait for controller-manager\"]\n        P4C --&gt; P4D[\"Verify CRD exists\"]\n    end\n\n    subgraph Phase5[\"Phase 5: GPU Node Labeling\"]\n        P5A[\"Query nodes for GPU capacity\"] --&gt; P5B[\"Label GPU nodes\"]\n        P5B --&gt; P5C[\"Log warning if no GPUs\"]\n    end\n\n    Phase1 --&gt; Phase2 --&gt; Phase3 --&gt; Phase4 --&gt; Phase5</code></pre>"},{"location":"architecture/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/#why-rawdeployment-mode","title":"Why RawDeployment Mode?","text":"<p>KServe supports two deployment modes: Serverless (using Knative) and RawDeployment (using standard Kubernetes Deployments). KFabrik currently uses RawDeployment for simplicity\u2014it's easier to set up, debug, and operate for local development.</p> <p>Knative support is planned for a future release.</p>"},{"location":"architecture/#why-sequential-installation","title":"Why Sequential Installation?","text":"<p>Parallel installation would be faster but creates race conditions. Istio requires Cert-Manager for TLS certificates. KServe requires Istio for ingress routing. The sequential approach adds approximately 2 minutes to installation time but eliminates entire categories of failure modes.</p>"},{"location":"architecture/#why-cleanup-before-kserve-installation","title":"Why Cleanup Before KServe Installation?","text":"<p>KServe's webhook configurations and CRDs can become orphaned when previous installations fail mid-process. The cleanup phase ensures idempotent behavior: running the installer multiple times produces the same result.</p>"},{"location":"architecture/#model-deployment-flow","title":"Model Deployment Flow","text":"<p>When a user runs <code>kfabrik deploy --models qwen-small</code>:</p> <pre><code>sequenceDiagram\n    participant CLI as kfabrik CLI\n    participant K8s as Kubernetes API\n    participant KServe as KServe Controller\n    participant Pod as Predictor Pod\n\n    CLI-&gt;&gt;K8s: Read model-config ConfigMap\n    K8s--&gt;&gt;CLI: Return configuration\n    CLI-&gt;&gt;CLI: Parse YAML, apply defaults\n    CLI-&gt;&gt;K8s: Check if InferenceService exists\n\n    alt Already exists\n        K8s--&gt;&gt;CLI: Skip (idempotent)\n    else Does not exist\n        CLI-&gt;&gt;K8s: Create InferenceService\n        K8s-&gt;&gt;KServe: Watch event\n        KServe-&gt;&gt;K8s: Create Deployment\n        KServe-&gt;&gt;K8s: Create Service\n        KServe-&gt;&gt;K8s: Create VirtualService\n        KServe-&gt;&gt;K8s: Create DestinationRule\n        K8s-&gt;&gt;Pod: Schedule pod\n        Pod-&gt;&gt;Pod: Download model from HuggingFace\n        Pod-&gt;&gt;Pod: Initialize inference server\n        Pod--&gt;&gt;K8s: Readiness probe succeeds\n        KServe-&gt;&gt;K8s: Update status Ready=True\n        K8s--&gt;&gt;CLI: Ready status (if --wait)\n    end</code></pre>"},{"location":"architecture/#inferenceservice-specification","title":"InferenceService Specification","text":"<p>The CLI generates InferenceService resources with the following structure:</p> <pre><code>apiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: qwen-small\n  namespace: model-serving\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: huggingface\n      storageUri: hf://Qwen/Qwen2.5-0.5B-Instruct\n      resources:\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n          nvidia.com/gpu: \"1\"\n        limits:\n          cpu: \"2\"\n          memory: \"4Gi\"\n          nvidia.com/gpu: \"1\"\n      env:\n        - name: HF_MODEL_ID\n          value: \"Qwen/Qwen2.5-0.5B-Instruct\"\n        - name: MAX_MODEL_LEN\n          value: \"2048\"\n        - name: DTYPE\n          value: \"float16\"\n</code></pre>"},{"location":"architecture/#monitoring-architecture","title":"Monitoring Architecture","text":"<pre><code>flowchart TB\n    subgraph monitoring[\"monitoring namespace\"]\n        Prometheus[\"Prometheus&lt;br/&gt;\u2022 Scrape jobs&lt;br/&gt;\u2022 Alert rules&lt;br/&gt;\u2022 TSDB storage\"]\n        Grafana[\"Grafana&lt;br/&gt;\u2022 Dashboards&lt;br/&gt;\u2022 Data sources&lt;br/&gt;\u2022 Alerting\"]\n        DCGM[\"DCGM Exporter&lt;br/&gt;(DaemonSet)&lt;br/&gt;GPU nodes only\"]\n\n        DCGM --&gt;|\"scrape :9400\"| Prometheus\n        Prometheus --&gt;|\"query :9090\"| Grafana\n    end\n\n    subgraph model-serving[\"model-serving namespace\"]\n        qwen-small[\"qwen-small&lt;br/&gt;predictor :9090\"]\n        qwen-medium[\"qwen-medium&lt;br/&gt;predictor :9090\"]\n        phi2[\"phi2&lt;br/&gt;predictor :9090\"]\n    end\n\n    qwen-small --&gt;|scrape| Prometheus\n    qwen-medium --&gt;|scrape| Prometheus\n    phi2 --&gt;|scrape| Prometheus</code></pre>"},{"location":"architecture/#prometheus-scrape-jobs","title":"Prometheus Scrape Jobs","text":"Scrape Job Targets Metrics prometheus Self (localhost:9090) Prometheus internal metrics kubernetes-apiservers API server API request latency, etcd metrics kubernetes-nodes All nodes Node CPU, memory, disk kubernetes-nodes-cadvisor All nodes Container resource usage kubernetes-service-endpoints Annotated services Custom service metrics kubernetes-pods Annotated pods Custom pod metrics dcgm-exporter GPU node pods GPU utilization, temperature, memory kserve-inferenceservices Model predictor pods Inference latency, throughput"},{"location":"architecture/#gpu-metrics-dcgm-exporter","title":"GPU Metrics (DCGM Exporter)","text":"Metric Type Description DCGM_FI_DEV_GPU_UTIL gauge GPU utilization percentage DCGM_FI_DEV_MEM_COPY_UTIL gauge Memory copy engine utilization DCGM_FI_DEV_FB_FREE gauge Free framebuffer memory (MiB) DCGM_FI_DEV_FB_USED gauge Used framebuffer memory (MiB) DCGM_FI_DEV_FB_TOTAL gauge Total framebuffer memory (MiB) DCGM_FI_DEV_GPU_TEMP gauge GPU temperature (Celsius) DCGM_FI_DEV_MEMORY_TEMP gauge Memory temperature (Celsius) DCGM_FI_DEV_POWER_USAGE gauge Power consumption (Watts)"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture/#privilege-requirements","title":"Privilege Requirements","text":"<p>The kfabrik-bootstrap installer Job requires cluster-admin privileges for:</p> <ul> <li>Creating CustomResourceDefinitions (cluster-scoped)</li> <li>Creating ClusterRoles and ClusterRoleBindings</li> <li>Creating ValidatingWebhookConfigurations</li> <li>Installing Helm charts that modify cluster-wide resources</li> </ul> <p>Mitigations:</p> <ul> <li>Installer Job has TTL of 300 seconds (auto-deleted after completion)</li> <li>ServiceAccount is scoped to kserve namespace</li> <li>Installer runs once during addon enablement, not continuously</li> </ul>"},{"location":"architecture/#network-exposure","title":"Network Exposure","text":"<p>By default, KFabrik does not expose services outside the cluster. All access occurs through:</p> <ul> <li><code>kubectl port-forward</code> for CLI queries</li> <li><code>minikube service</code> for Grafana/Prometheus dashboards</li> </ul>"},{"location":"architecture/#dependencies","title":"Dependencies","text":""},{"location":"architecture/#external-dependencies","title":"External Dependencies","text":"Component Source Purpose Cert-Manager Jetstack Helm TLS certificate management Istio Istio Helm Service mesh, ingress KServe KServe OCI Model serving platform NVIDIA Device Plugin NVIDIA GPU resource scheduling Prometheus Docker Hub Metrics collection Grafana Docker Hub Visualization DCGM Exporter NVIDIA GPU metrics"},{"location":"architecture/#runtime-dependencies","title":"Runtime Dependencies","text":"Dependency Required By Purpose minikube kfabrik CLI Cluster management kubectl kfabrik CLI Port-forwarding, API access Docker minikube Container runtime NVIDIA Driver Host GPU access NVIDIA Container Toolkit minikube GPU passthrough"},{"location":"architecture/#resource-requirements","title":"Resource Requirements","text":""},{"location":"architecture/#default-resource-requirements","title":"Default Resource Requirements","text":"Component CPU Request CPU Limit Memory Request Memory Limit KServe Controller 100m 500m 256Mi 512Mi Prometheus 250m 500m 512Mi 1Gi Grafana 100m 200m 256Mi 512Mi DCGM Exporter 100m 500m 512Mi 1Gi Model Predictor (small) 1 2 2Gi 4Gi Model Predictor (medium) 2 4 6Gi 10Gi"},{"location":"architecture/#port-assignments","title":"Port Assignments","text":"Service Port Protocol Purpose Prometheus 9090 HTTP Metrics API, Web UI Grafana 3000 HTTP Dashboard UI DCGM Exporter 9400 HTTP GPU metrics endpoint Model Predictor 80 HTTP Inference API Model Predictor Metrics 9090 HTTP Prometheus metrics"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>Complete reference for the kfabrik command-line interface.</p>"},{"location":"cli-reference/#synopsis","title":"Synopsis","text":"<pre><code>kfabrik [GLOBAL OPTIONS] command [COMMAND OPTIONS]\n</code></pre>"},{"location":"cli-reference/#global-options","title":"Global Options","text":"Option Description <code>-h, --help</code> Show help message and exit <code>--kubeconfig FILE</code> Path to kubeconfig file (default: $KUBECONFIG or ~/.kube/config) <code>-n, --namespace NAMESPACE</code> Kubernetes namespace for model deployments (default: model-serving)"},{"location":"cli-reference/#commands","title":"Commands","text":""},{"location":"cli-reference/#cluster","title":"cluster","text":"<p>Manage the minikube cluster lifecycle with GPU or CPU-only support.</p>"},{"location":"cli-reference/#cluster-start","title":"cluster start","text":"<p>Start a new minikube cluster configured for ML workloads.</p> <pre><code>kfabrik cluster start [FLAGS]\n</code></pre> <p>Flags:</p> Flag Description <code>--cpu-only</code> Force CPU-only mode, even if GPU is available <code>--driver NAME</code> Minikube driver to use (default: auto-detect) <code>--memory MB</code> Memory to allocate in megabytes (default: 32768) <code>--cpus N</code> Number of CPUs to allocate (default: 8) <code>--skip-model</code> Skip deploying the default model <code>--model NAME</code> Model to deploy (default: qwen-small) <p>Examples:</p> <pre><code># Start with GPU (auto-detected)\nkfabrik cluster start\n\n# Start in CPU-only mode\nkfabrik cluster start --cpu-only\n\n# Start with custom resources\nkfabrik cluster start --memory 16384 --cpus 4\n\n# Start without deploying a model\nkfabrik cluster start --skip-model\n</code></pre>"},{"location":"cli-reference/#cluster-stop","title":"cluster stop","text":"<p>Stop and delete the minikube cluster, cleaning up all resources.</p> <pre><code>kfabrik cluster stop\n</code></pre>"},{"location":"cli-reference/#deploy","title":"deploy","text":"<p>Deploy one or more LLM models to Kubernetes using KServe InferenceServices.</p> <pre><code>kfabrik deploy [FLAGS]\n</code></pre> <p>Flags:</p> Flag Description <code>--models MODEL[,MODEL...]</code> Comma-separated list of models to deploy <code>--all</code> Deploy all configured models <code>--wait</code> Wait for models to become ready before returning <p>Examples:</p> <pre><code># Deploy a single model\nkfabrik deploy --models qwen-small\n\n# Deploy multiple models\nkfabrik deploy --models qwen-small,tinyllama\n\n# Deploy and wait for readiness\nkfabrik deploy --models qwen-medium --wait\n</code></pre>"},{"location":"cli-reference/#delete","title":"delete","text":"<p>Delete one or more deployed models from Kubernetes.</p> <pre><code>kfabrik delete [FLAGS]\n</code></pre> <p>Flags:</p> Flag Description <code>--model NAME</code> Name of the model to delete <code>--all</code> Delete all deployed models <p>Examples:</p> <pre><code># Delete a specific model\nkfabrik delete --model qwen-small\n\n# Delete all models\nkfabrik delete --all\n</code></pre>"},{"location":"cli-reference/#list","title":"list","text":"<p>List models available for deployment or currently deployed.</p> <pre><code>kfabrik list [FLAGS]\n</code></pre> <p>Flags:</p> Flag Description <code>--available</code> List available models from configuration <code>--deployed</code> List currently deployed models with their status <p>Without flags, lists both available and deployed models.</p> <p>Example Output:</p> <pre><code>Available Models (all fit in 6GB VRAM):\n  NAME         PARAMS  VRAM   DOWNLOAD  DISPLAY NAME\n  qwen-small   0.5B    1GB    1GB       Qwen 2.5 0.5B Instruct\n  qwen-medium  1.5B    3GB    3GB       Qwen 2.5 1.5B Instruct\n  tinyllama    1.1B    2.5GB  2.2GB     TinyLlama 1.1B Chat\n  smollm2      1.7B    3.5GB  3.4GB     SmolLM2 1.7B Instruct\n  phi2         2.7B    5.5GB  5.5GB     Phi-2 (2.7B)\n\nDeployed Models:\n  NAME        READY  URL\n  qwen-small  True   http://qwen-small-model-serving.example.com\n</code></pre>"},{"location":"cli-reference/#status","title":"status","text":"<p>Check the status of deployed models.</p> <pre><code>kfabrik status [FLAGS]\n</code></pre> <p>Flags:</p> Flag Description <code>--model NAME</code> Name of the model to check <code>--all</code> Check status of all deployed models <code>-o, --output FORMAT</code> Output format: table (default) or json <p>Examples:</p> <pre><code># Check specific model\nkfabrik status --model qwen-small\n\n# Check all models as JSON\nkfabrik status --all --output json\n</code></pre>"},{"location":"cli-reference/#query","title":"query","text":"<p>Send an inference query to a deployed model using the OpenAI-compatible chat completions API.</p> <pre><code>kfabrik query [FLAGS]\n</code></pre> <p>Flags:</p> Flag Description <code>--model NAME</code> Name of the model to query (required) <code>--prompt TEXT</code> Prompt to send to the model (required) <code>--temperature FLOAT</code> Sampling temperature 0.0-2.0 (default: 0.7) <code>--max-tokens INT</code> Maximum tokens to generate (default: 256) <code>--top-p FLOAT</code> Top-p sampling parameter 0.0-1.0 (default: 0.9) <code>--timeout SECONDS</code> Request timeout in seconds (default: 120) <p>The command automatically sets up port-forwarding to the model's predictor service.</p> <p>Examples:</p> <pre><code># Basic query\nkfabrik query --model qwen-small --prompt \"What is Kubernetes?\"\n\n# Query with custom parameters\nkfabrik query --model qwen-medium \\\n  --prompt \"Explain machine learning in simple terms\" \\\n  --temperature 0.5 \\\n  --max-tokens 500\n</code></pre> <p>Example Output:</p> <pre><code>Model: qwen-small\nResponse:\nKubernetes is an open-source container orchestration platform...\n\n[Tokens: prompt=34, completion=156, total=190]\n</code></pre>"},{"location":"cli-reference/#logs","title":"logs","text":"<p>View logs for a deployed model's pods.</p> <pre><code>kfabrik logs [FLAGS]\n</code></pre> <p>Flags:</p> Flag Description <code>--model NAME</code> Name of the model (required) <code>-f, --follow</code> Stream log output (like tail -f) <code>--lines INT</code> Number of lines to show (default: 50) <code>-c, --container NAME</code> Container name (default: first container) <p>Examples:</p> <pre><code># View recent logs\nkfabrik logs --model qwen-small\n\n# Follow logs in real-time\nkfabrik logs --model qwen-small --follow\n\n# Show last 100 lines\nkfabrik logs --model qwen-small --lines 100\n</code></pre>"},{"location":"cli-reference/#version","title":"version","text":"<p>Print the version of kfabrik.</p> <pre><code>kfabrik version\n</code></pre>"},{"location":"cli-reference/#available-models","title":"Available Models","text":"<p>The following models are pre-configured and optimized for consumer GPUs with 6GB VRAM or less:</p> Name Parameters VRAM Download Description qwen-small 0.5B ~1GB ~1GB Qwen 2.5 0.5B Instruct qwen-medium 1.5B ~3GB ~3GB Qwen 2.5 1.5B Instruct tinyllama 1.1B ~2.5GB ~2.2GB TinyLlama 1.1B Chat smollm2 1.7B ~3.5GB ~3.4GB SmolLM2 1.7B Instruct phi2 2.7B ~5.5GB ~5.5GB Microsoft Phi-2 <p>Models are deployed one at a time; you don't need VRAM for all models simultaneously. System RAM requirements are approximately 2-3x the model size for inference server overhead.</p>"},{"location":"cli-reference/#environment-variables","title":"Environment Variables","text":"Variable Default Description KUBECONFIG ~/.kube/config Kubernetes configuration file KFABRIK_CPU_ONLY false Force CPU-only mode KFABRIK_MINIKUBE_BINARY minikube Path to minikube binary KFABRIK_KUBECTL_BINARY kubectl Path to kubectl binary"},{"location":"cli-reference/#exit-status","title":"Exit Status","text":"Code Description 0 Success 1 General error (invalid arguments, connection failed, etc.)"},{"location":"cli-reference/#see-also","title":"See Also","text":"<ul> <li>Getting Started - Installation and setup</li> <li>Addons - Addon configuration</li> <li>Architecture - How KFabrik works</li> </ul>"},{"location":"cncf-landscape/","title":"CNCF AI Landscape","text":"<p>KFabrik is part of the broader Cloud Native Computing Foundation (CNCF) ecosystem focused on AI and machine learning workloads. This page provides context on related projects and how they work together.</p>"},{"location":"cncf-landscape/#related-cncf-projects","title":"Related CNCF Projects","text":""},{"location":"cncf-landscape/#model-serving-inference","title":"Model Serving &amp; Inference","text":"<p>KServe Kubernetes-native model serving platform for machine learning models. Provides serverless inference with autoscaling, canary deployments, and multi-framework support.</p> <p>vLLM High-throughput and memory-efficient inference engine for large language models. Optimized for serving LLMs at scale with advanced batching techniques.</p>"},{"location":"cncf-landscape/#orchestration-management","title":"Orchestration &amp; Management","text":"<p>Karmada Multi-cluster Kubernetes management system that enables workload distribution across multiple clusters, useful for distributed AI training and inference.</p> <p>LLMD Kubernetes operator for managing large language model deployments and lifecycle operations.</p>"},{"location":"cncf-landscape/#how-kfabrik-fits-in","title":"How KFabrik Fits In","text":"<p>KFabrik complements these projects by:</p> <ul> <li>Simplifying deployment of AI workloads across the CNCF ecosystem</li> <li>Providing abstractions that work with KServe, vLLM, and other inference engines</li> <li>Enabling multi-cluster AI workflows through integration with Karmada</li> <li>Democratizing access to cloud-native AI tools for developers</li> </ul>"},{"location":"cncf-landscape/#integration-patterns","title":"Integration Patterns","text":""},{"location":"cncf-landscape/#with-kserve","title":"With KServe","text":"<p>KFabrik can deploy models that are served by KServe, providing a simplified interface for model deployment and management.</p>"},{"location":"cncf-landscape/#with-vllm","title":"With vLLM","text":"<p>Use KFabrik to orchestrate vLLM deployments for high-performance LLM serving in local development environments.</p>"},{"location":"cncf-landscape/#with-karmada","title":"With Karmada","text":"<p>Leverage Karmada's multi-cluster capabilities to distribute KFabrik-managed AI workloads across different regions or cloud providers.</p>"},{"location":"cncf-landscape/#learn-more","title":"Learn More","text":"<ul> <li>CNCF AI Working Group</li> <li>CNCF Landscape - AI/ML Category</li> <li>KServe GitHub Organization</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>KFabrik is an open-source project and welcomes contributions from the community. As the project is in early development, now is a great time to get involved and help shape its direction.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Improve guides and tutorials</li> <li>Add examples and use cases</li> <li>Fix typos and clarify explanations</li> <li>Translate content</li> </ul>"},{"location":"contributing/#code-coming-soon","title":"Code (Coming Soon)","text":"<p>Once the software is released, you can contribute: - Features and enhancements - Bug fixes - Integration tests - Performance optimizations</p>"},{"location":"contributing/#community","title":"Community","text":"<ul> <li>Answer questions from other users</li> <li>Share your use cases and feedback</li> <li>Present at meetups or conferences</li> <li>Help with testing and validation</li> </ul>"},{"location":"contributing/#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"contributing/#writing-style","title":"Writing Style","text":"<ul> <li>Use clear, concise language</li> <li>Write in present tense</li> <li>Use active voice when possible</li> <li>Keep paragraphs short and focused</li> </ul>"},{"location":"contributing/#markdown-formatting","title":"Markdown Formatting","text":"<ul> <li>Use proper heading hierarchy (h1 \u2192 h2 \u2192 h3)</li> <li>Include code blocks with language identifiers for syntax highlighting</li> <li>Use relative links for internal documentation references</li> <li>Add alt text for images</li> </ul>"},{"location":"contributing/#code-examples","title":"Code Examples","text":"<p>When the software is available: - Provide complete, working examples - Include expected output - Explain what the code does - Use realistic but simple scenarios</p>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/your-feature-name</code>)</li> <li>Make your changes</li> <li>Test locally with <code>mkdocs serve</code></li> <li>Commit with clear, descriptive messages</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#file-organization","title":"File Organization","text":"<ul> <li>Place all documentation in the <code>docs/</code> directory</li> <li>Use lowercase filenames with hyphens (e.g., <code>getting-started.md</code>)</li> <li>Update <code>mkdocs.yml</code> navigation when adding new pages</li> <li>Keep related content together</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your changes follow the documentation standards</li> <li>Update the navigation in <code>mkdocs.yml</code> if adding new pages</li> <li>Test that all links work correctly</li> <li>Provide a clear description of your changes</li> <li>Reference any related issues</li> </ol>"},{"location":"contributing/#community-resources","title":"Community Resources","text":"<ul> <li>GitHub: github.com/kfabrik</li> <li>Community: github.com/kfabrik/community</li> <li>Slack: Coming soon</li> <li>Mailing List: Coming soon</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>KFabrik is committed to providing a welcoming and inclusive environment. All contributors are expected to:</p> <ul> <li>Be respectful and considerate</li> <li>Welcome newcomers and help them get started</li> <li>Focus on constructive feedback</li> <li>Respect differing viewpoints and experiences</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to KFabrik, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>If you have questions about contributing, please open an issue on GitHub or reach out through community channels.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through setting up KFabrik and deploying your first LLM model.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/#required-software","title":"Required Software","text":"<ul> <li>Go 1.21 or later (for building)</li> <li>Make (for building)</li> <li>Docker with NVIDIA Container Toolkit (for GPU support)</li> <li>kubectl (installed automatically with minikube)</li> </ul>"},{"location":"getting-started/#for-gpu-support-linux-only","title":"For GPU Support (Linux only)","text":"<ul> <li>NVIDIA GPU with drivers installed on host</li> <li>NVIDIA Container Toolkit configured for Docker</li> </ul>"},{"location":"getting-started/#platform-support","title":"Platform Support","text":"Platform GPU Support Notes Linux with NVIDIA GPU Full Default configuration Linux without GPU CPU-only Auto-detected macOS Coming soon Windows Coming soon"},{"location":"getting-started/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/#minimum-requirements","title":"Minimum Requirements","text":"Resource Minimum CPU 4 cores Memory 8GB RAM Disk 40GB"},{"location":"getting-started/#recommended-for-gpu-workloads","title":"Recommended for GPU Workloads","text":"Resource Recommended CPU 8+ cores Memory 16-32GB RAM Disk 50GB+ GPU VRAM 6GB+"},{"location":"getting-started/#installation","title":"Installation","text":"<p>KFabrik is distributed as a custom build of minikube. Clone the repository, build, and install.</p>"},{"location":"getting-started/#build-requirements","title":"Build Requirements","text":"<ul> <li>Go 1.21 or later</li> <li>Make</li> <li>Docker</li> </ul>"},{"location":"getting-started/#clone-and-build","title":"Clone and Build","text":"<pre><code># Clone the kfabrik minikube repository\ngit clone https://github.com/kfabrik/minikube.git\ncd minikube\n\n# Build minikube with kfabrik addons\nmake build\n\n# Install kfabrik CLI and minikube\n./scripts/install.sh\n</code></pre> <p>The install script places both <code>minikube</code> and <code>kfabrik</code> binaries in <code>/usr/local/bin</code>.</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#1-start-the-cluster","title":"1. Start the Cluster","text":"<pre><code># Start with GPU support (auto-detected)\nkfabrik cluster start\n\n# Or start in CPU-only mode\nkfabrik cluster start --cpu-only\n\n# Custom resource allocation\nkfabrik cluster start --memory 16384 --cpus 4\n</code></pre> <p>The <code>cluster start</code> command:</p> <ol> <li>Starts minikube with Docker driver and GPU passthrough</li> <li>Enables kfabrik-bootstrap addon (installs KServe, Istio, Cert-Manager)</li> <li>Enables kfabrik-model addon (creates model configurations)</li> <li>Enables kfabrik-monitoring addon (deploys Prometheus, Grafana)</li> <li>Deploys the default model (qwen-small)</li> </ol>"},{"location":"getting-started/#2-list-available-models","title":"2. List Available Models","text":"<pre><code>kfabrik list\n</code></pre> <p>Output: <pre><code>Available Models (all fit in 6GB VRAM):\n  NAME         PARAMS  VRAM   DOWNLOAD  DISPLAY NAME\n  qwen-small   0.5B    1GB    1GB       Qwen 2.5 0.5B Instruct\n  qwen-medium  1.5B    3GB    3GB       Qwen 2.5 1.5B Instruct\n  tinyllama    1.1B    2.5GB  2.2GB     TinyLlama 1.1B Chat\n  smollm2      1.7B    3.5GB  3.4GB     SmolLM2 1.7B Instruct\n  phi2         2.7B    5.5GB  5.5GB     Phi-2 (2.7B)\n\nDeployed Models:\n  NAME        READY  URL\n  qwen-small  True   http://qwen-small-model-serving.example.com\n</code></pre></p>"},{"location":"getting-started/#3-deploy-a-model","title":"3. Deploy a Model","text":"<pre><code># Deploy a single model\nkfabrik deploy --models qwen-small\n\n# Deploy and wait for readiness\nkfabrik deploy --models qwen-medium --wait\n\n# Deploy multiple models\nkfabrik deploy --models qwen-small,tinyllama\n</code></pre>"},{"location":"getting-started/#4-check-model-status","title":"4. Check Model Status","text":"<pre><code>kfabrik status --model qwen-small\n</code></pre> <p>Output: <pre><code>NAME        READY  REASON  URL\nqwen-small  True   Ready   http://qwen-small-model-serving.example.com\n</code></pre></p>"},{"location":"getting-started/#5-query-a-model","title":"5. Query a Model","text":"<pre><code>kfabrik query --model qwen-small --prompt \"What is Kubernetes?\"\n</code></pre> <p>Output: <pre><code>Model: qwen-small\nResponse:\nKubernetes is an open-source container orchestration platform...\n\n[Tokens: prompt=34, completion=156, total=190]\n</code></pre></p> <p>Query with custom parameters: <pre><code>kfabrik query --model qwen-medium \\\n  --prompt \"Explain machine learning in simple terms\" \\\n  --temperature 0.5 \\\n  --max-tokens 500\n</code></pre></p>"},{"location":"getting-started/#6-view-logs","title":"6. View Logs","text":"<pre><code># View recent logs\nkfabrik logs --model qwen-small\n\n# Follow logs in real-time\nkfabrik logs --model qwen-small --follow\n\n# Show last 100 lines\nkfabrik logs --model qwen-small --lines 100\n</code></pre>"},{"location":"getting-started/#7-delete-models","title":"7. Delete Models","text":"<pre><code># Delete a specific model\nkfabrik delete --model qwen-small\n\n# Delete all models\nkfabrik delete --all\n</code></pre>"},{"location":"getting-started/#8-stop-the-cluster","title":"8. Stop the Cluster","text":"<pre><code>kfabrik cluster stop\n</code></pre>"},{"location":"getting-started/#accessing-dashboards","title":"Accessing Dashboards","text":""},{"location":"getting-started/#grafana","title":"Grafana","text":"<p><pre><code>kubectl port-forward -n monitoring svc/grafana 3000:3000\n</code></pre> Open http://localhost:3000 (Login: admin / admin)</p>"},{"location":"getting-started/#prometheus","title":"Prometheus","text":"<p><pre><code>kubectl port-forward -n monitoring svc/prometheus 9090:9090\n</code></pre> Open http://localhost:9090</p>"},{"location":"getting-started/#manual-addon-management","title":"Manual Addon Management","text":"<p>If you prefer to manage addons separately (after building and installing):</p> <pre><code># Start minikube with GPU support\nminikube start --driver=docker --memory=32768 --cpus=8 --gpus=all\n\n# Enable core infrastructure\nminikube addons enable kfabrik-bootstrap\n\n# Wait for installation to complete\nkubectl wait --for=condition=complete job/kfabrik-installer \\\n  -n kserve --timeout=600s\n\n# Enable model configurations\nminikube addons enable kfabrik-model\n\n# Enable monitoring (optional)\nminikube addons enable kfabrik-monitoring\n\n# Deploy a model\nkfabrik deploy --models qwen-small --wait\n</code></pre>"},{"location":"getting-started/#environment-variables","title":"Environment Variables","text":"Variable Default Description KUBECONFIG ~/.kube/config Kubernetes configuration file KFABRIK_CPU_ONLY false Force CPU-only mode KFABRIK_MINIKUBE_BINARY minikube Path to minikube binary KFABRIK_KUBECTL_BINARY kubectl Path to kubectl binary"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#gpu-not-detected","title":"GPU Not Detected","text":"<pre><code># Verify NVIDIA driver on host\nnvidia-smi\n\n# Check device plugin logs\nkubectl logs -n kube-system -l name=nvidia-device-plugin-ds\n\n# Verify GPU is advertised\nkubectl describe node minikube | grep nvidia.com/gpu\n</code></pre>"},{"location":"getting-started/#model-fails-to-become-ready","title":"Model Fails to Become Ready","text":"<pre><code># Check InferenceService status\nkfabrik status --model &lt;name&gt;\n\n# Check predictor pod logs\nkfabrik logs --model &lt;name&gt;\n\n# Check KServe controller logs\nkubectl logs -n kserve -l control-plane=kserve-controller-manager\n</code></pre>"},{"location":"getting-started/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Increase minikube memory allocation:</p> <pre><code>minikube stop\nminikube config set memory 32768\nminikube start --gpus=all\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference - Complete command documentation</li> <li>Architecture - Understand how KFabrik works</li> <li>Addons - Addon configuration and customization</li> </ul>"},{"location":"prfaq/","title":"KFabrik PRFAQ","text":""},{"location":"prfaq/#press-release","title":"PRESS RELEASE","text":""},{"location":"prfaq/#kfabrik-launches-open-source-platform-for-local-llm-development-on-kubernetes","title":"KFabrik Launches Open-Source Platform for Local LLM Development on Kubernetes","text":"<p>Minikube addons and CLI enable local LLM development environments in under 10 minutes</p> <p>TAMPA, FL \u2014 February 2026 \u2014 The KFabrik project today announced the release of its open-source platform for local development and testing of Large Language Models (LLMs) on Kubernetes. KFabrik enables ML developers to deploy LLM inference servers on minikube for development purposes with a single command, eliminating weeks of manual configuration.</p> <p>The Problem: Infrastructure Friction Slows ML Development</p> <p>ML developers face significant friction when testing LLM deployments locally. The current state requires manual installation and configuration of multiple interdependent components: container runtimes with GPU support, Kubernetes clusters, service mesh infrastructure, model serving frameworks, and monitoring stacks.</p> <p>Deploying a single model requires creating and maintaining dozens of Kubernetes manifests across multiple namespaces. Developers must understand the intricacies of KServe InferenceServices, Istio VirtualServices, Prometheus scrape configurations, and GPU resource scheduling.</p> <p>The ML serving stack has strict ordering requirements. Cert-manager must be operational before Istio can create TLS certificates. Istio must be running before KServe can configure ingress routing. NVIDIA device plugins must register GPUs before model pods can request GPU resources. Manual installation frequently fails due to race conditions or missing prerequisites.</p> <p>\"We saw talented ML engineers spending days configuring infrastructure instead of building AI applications,\" said a KFabrik contributor. \"Every project required the same tedious setup: installing Cert-Manager, then Istio, then KServe, then debugging why they don't work together. KFabrik eliminates this entirely.\"</p> <p>The Solution: Integrated ML Inference Platform</p> <p>KFabrik solves these problems by providing an opinionated, fully-integrated ML inference platform for local development that deploys with a single command.</p> <p>The platform consists of three minikube addons and a command-line interface:</p> <ul> <li>kfabrik-bootstrap: Installs Cert-Manager, Istio, KServe, and NVIDIA Device Plugin in the correct dependency order</li> <li>kfabrik-model: Provides pre-configured LLM definitions optimized for consumer GPUs (6GB VRAM)</li> <li>kfabrik-monitoring: Deploys Prometheus, Grafana, and GPU metrics with DCGM Exporter</li> <li>kfabrik CLI: Provides commands for cluster management, model deployment, and inference queries</li> </ul> <p>How It Works</p> <p>Developers run a single command to start a GPU-enabled cluster with the complete ML serving stack:</p> <pre><code>kfabrik cluster start\nkfabrik deploy --models qwen-small --wait\nkfabrik query --model qwen-small --prompt \"What is Kubernetes?\"\n</code></pre> <p>A typical deployment takes under 10 minutes. The consistent local environment means all team members work with identical configurations, making it easy to share and reproduce issues.</p> <p>\"KFabrik transformed our team's workflow,\" said an early adopter. \"Junior ML engineers who previously needed weeks of Kubernetes training can now deploy and test models in their first hour. The consistent local setup means everyone on the team has the same development environment.\"</p> <p>Built for GPU-First Workflows</p> <p>KFabrik assumes GPU workloads are the primary use case. Resource defaults are tuned for NVIDIA GPUs with 6GB+ VRAM. Five pre-configured models are included:</p> <ul> <li>qwen-small (0.5B parameters, ~1GB VRAM) - Quick testing</li> <li>qwen-medium (1.5B parameters, ~3GB VRAM) - Balanced performance</li> <li>tinyllama (1.1B parameters, ~2.5GB VRAM) - Lightweight inference</li> <li>smollm2 (1.7B parameters, ~3.5GB VRAM) - Efficient small model</li> <li>phi2 (2.7B parameters, ~5.5GB VRAM) - Higher quality output</li> </ul> <p>Design Principles</p> <p>KFabrik follows five guiding tenets:</p> <ol> <li>Simplicity over flexibility: Optimized for the common case of deploying HuggingFace models on GPU-enabled minikube clusters</li> <li>Consistent environments: All developers get identical local configurations</li> <li>Explicit over implicit: All configuration is visible and auditable as standard Kubernetes resources</li> <li>Fast feedback: Parallel installations, aggressive health checks, and clear progress reporting</li> <li>GPU-first: Design decisions favor GPU scheduling efficiency</li> </ol> <p>Availability</p> <p>KFabrik is available now under the Apache 2.0 license. It is distributed as a custom build of minikube available at github.com/kfabrik/minikube.</p> <ul> <li>Documentation: kfabrik.io</li> <li>Source: github.com/kfabrik/minikube</li> </ul>"},{"location":"prfaq/#frequently-asked-questions","title":"FREQUENTLY ASKED QUESTIONS","text":""},{"location":"prfaq/#general-questions","title":"General Questions","text":"<p>Q: What is KFabrik?</p> <p>A: KFabrik is an integrated platform for deploying and managing Large Language Models (LLMs) on local Kubernetes clusters. It consists of three minikube addons (kfabrik-bootstrap, kfabrik-model, kfabrik-monitoring) and a command-line interface (kfabrik CLI) that together provide a complete ML inference stack optimized for GPU workloads.</p> <p>Q: Who is KFabrik for?</p> <p>A: KFabrik targets ML developers who need to experiment with and test LLM deployments locally. This includes:</p> <ol> <li>ML Engineers: Building and testing inference pipelines</li> <li>Data Scientists: Experimenting with different models locally</li> <li>Platform Engineers: Developing CI/CD pipelines for ML workloads</li> <li>Developers: Integrating LLM capabilities into applications</li> </ol> <p>Q: What problem does KFabrik solve?</p> <p>A: KFabrik eliminates the infrastructure friction in local ML development:</p> <ul> <li>Configuration burden: Dozens of Kubernetes manifests reduced to a single command</li> <li>Dependency management: Correct installation order handled automatically</li> <li>Observability gap: GPU metrics and model performance monitoring included</li> <li>Reproducibility: Consistent configurations that work identically across all developer machines</li> </ul>"},{"location":"prfaq/#technical-questions","title":"Technical Questions","text":"<p>Q: What are the system requirements?</p> <p>A: Minimum requirements:</p> <ul> <li>CPU: 4 cores (8 recommended)</li> <li>RAM: 8GB (16-32GB recommended for GPU workloads)</li> <li>Disk: 40GB</li> <li>GPU: NVIDIA GPU with 6GB+ VRAM (optional but recommended)</li> <li>OS: Linux (macOS and Windows support coming soon)</li> </ul> <p>Q: What components does KFabrik install?</p> <p>A: KFabrik installs the following components:</p> Component Purpose Cert-Manager TLS certificate management Istio Service mesh and ingress KServe Model serving platform NVIDIA Device Plugin GPU resource scheduling Prometheus Metrics collection Grafana Visualization DCGM Exporter GPU metrics <p>Q: How does KFabrik handle GPU support?</p> <p>A: KFabrik automatically detects GPU availability and configures the cluster accordingly:</p> <ul> <li>Linux with NVIDIA GPU: Full GPU acceleration (default)</li> <li>Linux without GPU: CPU-only mode (auto-detected)</li> <li>macOS: Coming soon</li> <li>Windows: Coming soon</li> </ul> <p>The <code>--cpu-only</code> flag can force CPU-only mode even when a GPU is available.</p> <p>Q: What models are pre-configured?</p> <p>A: Five models optimized for consumer GPUs (6GB VRAM):</p> Model Parameters VRAM Use Case qwen-small 0.5B ~1GB Quick testing, low latency qwen-medium 1.5B ~3GB Balanced performance tinyllama 1.1B ~2.5GB Lightweight inference smollm2 1.7B ~3.5GB Efficient small model phi2 2.7B ~5.5GB Higher quality, more resources <p>Q: Can I use custom models?</p> <p>A: Yes. KFabrik supports any HuggingFace model. Create a custom configuration file specifying the model URI and resource requirements, then deploy with the CLI.</p> <p>Q: How does the query command work?</p> <p>A: The <code>kfabrik query</code> command:</p> <ol> <li>Sets up kubectl port-forwarding to the model's predictor service</li> <li>Sends an OpenAI-compatible chat completion request</li> <li>Displays the response with token usage statistics</li> <li>Cleans up the port-forward</li> </ol> <p>Q: Why does KFabrik use RawDeployment mode instead of Knative?</p> <p>A: KServe supports two deployment modes: Serverless (using Knative) and RawDeployment (using standard Kubernetes Deployments). KFabrik currently uses RawDeployment for simplicity\u2014it's easier to set up, debug, and operate for local development.</p> <p>Knative support is planned for a future release.</p>"},{"location":"prfaq/#operations-questions","title":"Operations Questions","text":"<p>Q: How long does deployment take?</p> <p>A: A typical deployment takes under 10 minutes:</p> <ul> <li>Cluster start with addons: ~5-7 minutes</li> <li>Model deployment (first time): ~3-5 minutes (includes model download)</li> <li>Model deployment (cached): ~1-2 minutes</li> </ul> <p>Q: How do I troubleshoot deployment issues?</p> <p>A: Use the following commands:</p> <pre><code># Check installer progress\nkubectl get jobs -n kserve\nkubectl logs -n kserve -l job-name=kfabrik-installer\n\n# Check model status\nkfabrik status --model &lt;name&gt;\nkfabrik logs --model &lt;name&gt;\n\n# Check KServe controller\nkubectl logs -n kserve -l control-plane=kserve-controller-manager\n\n# Verify GPU detection\nkubectl describe node minikube | grep nvidia.com/gpu\n</code></pre> <p>Q: How do I access monitoring dashboards?</p> <p>A: Use kubectl port-forward:</p> <pre><code># Grafana (login: admin/admin)\nkubectl port-forward -n monitoring svc/grafana 3000:3000\n\n# Prometheus\nkubectl port-forward -n monitoring svc/prometheus 9090:9090\n</code></pre> <p>Q: Can I use KFabrik for production?</p> <p>A: No. KFabrik is designed exclusively for local development and testing. It should not be used for production deployments. For production workloads, use appropriate tooling designed for multi-node clusters, high availability, and enterprise operations.</p>"},{"location":"prfaq/#comparison-questions","title":"Comparison Questions","text":"<p>Q: How does KFabrik compare to deploying KServe manually?</p> <p>A: Manual KServe deployment requires:</p> <ol> <li>Installing Cert-Manager (with correct CRD configuration)</li> <li>Installing Istio (with correct IngressClass setup)</li> <li>Installing KServe (with correct deployment mode)</li> <li>Configuring GPU support (device plugins, node labels)</li> <li>Setting up monitoring (Prometheus, Grafana, DCGM)</li> </ol> <p>This process typically takes 2-4 weeks to get right. KFabrik handles all of this in under 10 minutes with tested, compatible configurations.</p> <p>Q: How does KFabrik compare to cloud-managed ML services?</p> <p>A: Cloud services (SageMaker, Vertex AI, Azure ML) are excellent for production but:</p> <ul> <li>Create vendor lock-in with proprietary APIs</li> <li>Expensive for persistent development environments</li> <li>Slow iteration cycles requiring cloud deployment</li> <li>Require internet connectivity</li> </ul> <p>KFabrik provides local, open-source environments using the same KServe APIs that work across any Kubernetes cluster.</p> <p>Q: Why minikube addons instead of a standalone installer?</p> <p>A: Minikube addons provide:</p> <ol> <li>Dependency management: Addons can declare dependencies on other addons</li> <li>Lifecycle integration: Enable/disable with standard minikube commands</li> <li>GPU support: Minikube handles GPU passthrough configuration</li> <li>Familiar tooling: Developers already know minikube workflows</li> </ol> <p>Document Version: 2.0 Last Updated: February 2026 Status: Current</p>"}]}